{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import funcy\n",
    "import os\n",
    "import re\n",
    "from dev import LOCAL_DB, DATA_DIR\n",
    "from models import Commit, Change, Developer, Diff\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_DIR = os.path.join(DATA_DIR, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import connect_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lraymond/Python/sourcecred_research/sourcecred_research3.6/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "session, engine = connect_to_db(LOCAL_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of commits per developer over time - get a dataset of filename new,\n",
    "# filename old, commit id, commit date, commit author, committer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dict = {\n",
    "    'filename_new': [],\n",
    "    'filename_old': [],\n",
    "    'commit_id': None,\n",
    "    'additions': 0,\n",
    "    'deletions': 0,\n",
    "    'is_deletion': False,\n",
    "    'is_rename': False,\n",
    "    'is_new': False,\n",
    "    'commit_timestamp': None,\n",
    "    'commit_subject': '',\n",
    "    'commit_body': '',\n",
    "    'author_id': None,\n",
    "    'commit_id': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of unique filenames\n",
    "# filename_tups = session.query().with_entities(\n",
    "#     Diff.filename_new, Diff.filename_old, Diff.commit_id, \n",
    "#         Diff.additions, Diff.deletions, Diff.is_deletion, Diff.is_rename, \n",
    "#             Diff.is_new).all()\n",
    "\n",
    "filename_tups = session.query(Diff).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame(list(map(lambda x: x.__dict__, filename_tups)))\n",
    "diff_df.rename(columns={'id': 'diff_id'}, inplace=True)\n",
    "diff_df2 = diff_df.drop_duplicates(subset=['filename_new', 'filename_old', 'commit_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10762, 12)\n",
      "(4739, 12)\n"
     ]
    }
   ],
   "source": [
    "print(diff_df.shape)\n",
    "print(diff_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del diff_df2['_sa_instance_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_df = pd.DataFrame(list(map(lambda x: x.__dict__, session.query(Commit).all())))\n",
    "commits_df.rename(columns={'id': 'commit_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del commits_df['_sa_instance_state']\n",
    "del commits_df['raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4739, 20)\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(left=diff_df2, right=commits_df, on='commit_id')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in author information\n",
    "author_df = pd.DataFrame(list(map(lambda x: x.__dict__, session.query(Developer).all())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del author_df['_sa_instance_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in author\n",
    "auth2 = author_df.rename(columns={'email': 'author_email', 'id': 'author_id'})\n",
    "df2 = df.merge(right=auth2[['author_email', 'author_id']], on='author_id')\n",
    "auth3 = author_df.rename(columns={'email': 'commiter_email', 'id': 'commiter_id'})\n",
    "df3 = df2.merge(right=auth3[['commiter_email', 'commiter_id']], on='commiter_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del auth2, auth3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of renames (98, 22)\n",
      "Total shape  (4739, 22)\n"
     ]
    }
   ],
   "source": [
    "# Here I should create something that looks for file renames and maps old name to new name and creates a new column (stable filename)\n",
    "print('Number of renames', df3.loc[df3.is_rename==True].shape)\n",
    "print('Total shape ', df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the ones with blank files names, search for \"rename from \" to \"rename to\"\n",
    "def find_renames(raw_diff_text):\n",
    "    m = re.search(r'rename from (.+\\.\\w{1,10})(\\|\\|\\|\\||\\|\\|\\|)rename to (.+\\.\\w{1,10})', raw_diff_text)\n",
    "    if m:\n",
    "        res = m.groups()\n",
    "        if len(res) == 3:\n",
    "            return res[0], res[2]\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4739,)\n"
     ]
    }
   ],
   "source": [
    "missing_filenames = ((df3.is_rename==1) & (df3.filename_new=='') & (df3.filename_old==''))\n",
    "print(missing_filenames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_names = df3['raw_diff'].apply(find_renames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[missing_filenames, 'filename_old'] = replacement_names[missing_filenames].apply(lambda x: x[0])\n",
    "df3.loc[missing_filenames, 'filename_new'] = replacement_names[missing_filenames].apply(lambda x: x[1])\n",
    "df3[['is_deletion', 'is_new', 'is_rename']] = df3[['is_deletion', 'is_new', 'is_rename']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if fillin gin the renames worked\n",
    "# df3.loc[missing_filenames, ['filename_new', 'filename_old', 'raw_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of tups of all filename matches (old, new)\n",
    "filename_tups = df3[['timestamp', 'filename_old', 'filename_new']].sort_values('timestamp').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/test-repo/blocks/1220c0fc/1220c0fc6b49543d7bf04e83d2a5a7cbe72a83e80f9c7bca1abcaa42298a57a33ff5.data tests/repo-example/blocks/1220c0fc/1220c0fc6b49543d7bf04e83d2a5a7cbe72a83e80f9c7bca1abcaa42298a57a33ff5.data\n",
      "tests/repo-example/blocks/1220933b/1220933b41d37fd4508cdff45930dff56baef91c7dc345e73d049ab570abe10dfbb9.data test/go-ipfs-repo/blocks/1220933b/1220933b41d37fd4508cdff45930dff56baef91c7dc345e73d049ab570abe10dfbb9.data\n",
      "test/http-api/test-bitswap.js test/http-api/ipfs-api/test-bitswap.js\n",
      "test/go-ipfs-repo/blocks/1220b0cb/1220b0cba7371f11461f77081b947d837cc9a3b80e182ac123bbd427563e8b167c96.data test/go-ipfs-repo/blocks/CIQLB/CIQLBS5HG4PRCRQ7O4EBXFD5QN6MTI5YBYMCVQJDXPKCOVR6RMLHZFQ.data\n",
      "test/http-api/spec/test-pubsub.js test/http-api/spec/pubsub.js\n",
      "test/http-api/spec/test-config.js test/http-api/spec/config.js\n",
      "test/go-ipfs-repo/blocks/CIQER/CIQERMRAAFXUAUOX3V2DCW7R77FRIVHQ3V5OIPPS3XQBX34KRPNOIRQ.data test/test-data/go-ipfs-repo/blocks/CIQER/CIQERMRAAFXUAUOX3V2DCW7R77FRIVHQ3V5OIPPS3XQBX34KRPNOIRQ.data\n",
      "test/http-api/interface/test-object.js test/http-api/interface/object.js\n",
      "test/http-api/interface/test-files.js test/http-api/interface/files.js\n",
      "test/http-api/interface/test-config.js test/http-api/interface/config.js\n",
      "test/http-api/spec/test-bitswap.js test/http-api/spec/bitswap.js\n",
      "test/http-api/interface/test-block.js test/http-api/interface/block.js\n",
      "test/http-api/over-ipfs-api/test-block.js test/http-api/over-ipfs-api/block.js\n",
      "test/http-api/interface/test-pubsub.js test/http-api/interface/pubsub.js\n",
      "test/http-api/interface/test-swarm.js test/http-api/interface/swarm.js\n",
      "test/test-data/go-ipfs-repo/blocks/CIQHP/CIQHPUVCWD6JA6AFUVD6VA64TGWP67KYA3AIMBUMVWGZ5AQN2L2HSWQ.data test/go-ipfs-repo/blocks/SW/CIQHPUVCWD6JA6AFUVD6VA64TGWP67KYA3AIMBUMVWGZ5AQN2L2HSWQ.data\n",
      "test/go-ipfs-repo/blocks/M4/CIQOLBQZSZAODJGGH6RYYVBUXHTS3SM5EORZDU63LYPEFUAFE4SBM4I.data test/fixtures/go-ipfs-repo/blocks/M4/CIQOLBQZSZAODJGGH6RYYVBUXHTS3SM5EORZDU63LYPEFUAFE4SBM4I.data\n",
      "test/http-api/extra/object.js test/http-api/object.js\n",
      "test/http-api/extra/config.js test/http-api/config.js\n"
     ]
    }
   ],
   "source": [
    "canonical_name_to_other_names_dict = dict()\n",
    "# dict that goes from canonical names to other file names\n",
    "\n",
    "other_name_to_canonical_name_dict = dict()\n",
    "# dict that does from file names to canonical name\n",
    "\n",
    "for time_val, old_name, new_name in filename_tups:\n",
    "    if old_name == '/dev/null':\n",
    "        # this is a file creation\n",
    "        if new_name not in canonical_name_to_other_names_dict:\n",
    "            canonical_name_to_other_names_dict[new_name] = [new_name]\n",
    "            other_name_to_canonical_name_dict[new_name] = new_name\n",
    "            \n",
    "    elif new_name == '/dev/null':\n",
    "        # this is a file deletion\n",
    "        # need to check that it exists in the dict because not totally capturing everything perfectly\n",
    "        if old_name not in other_name_to_canonical_name_dict:\n",
    "            # we haven't done anything with this filename yet\n",
    "            canonical_name_to_other_names_dict[old_name] = [old_name]\n",
    "            other_name_to_canonical_name_dict[old_name] = old_name\n",
    "            \n",
    "    elif new_name==old_name:\n",
    "        # new name is the same as old_name\n",
    "         if new_name not in other_name_to_canonical_name_dict:\n",
    "            # we haven't done anything with this filename yet\n",
    "            canonical_name_to_other_names_dict[new_name] = [new_name]\n",
    "            other_name_to_canonical_name_dict[new_name] = new_name\n",
    "    elif new_name != old_name:\n",
    "        # and neither new name nor old name is dev/null\n",
    "        if old_name in other_name_to_canonical_name_dict:\n",
    "            can_name = other_name_to_canonical_name_dict[old_name]\n",
    "            if can_name not in canonical_name_to_other_names_dict:\n",
    "                can_name = other_name_to_canonical_name_dict[can_name]\n",
    "            canonical_name_to_other_names_dict[can_name].append(new_name)\n",
    "            other_name_to_canonical_name_dict[new_name] = old_name\n",
    "        else:\n",
    "            # this is a weird pair\n",
    "            print(old_name, new_name) \n",
    "            other_name_to_canonical_name_dict[old_name] = old_name\n",
    "            other_name_to_canonical_name_dict[new_name] = old_name\n",
    "            canonical_name_to_other_names_dict[old_name] = [new_name]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing found\n",
      "test/core-tests/test-init.js test/core/both/test-init.js\n",
      "Nothing found\n",
      "examples/basics/index.js /dev/null\n"
     ]
    }
   ],
   "source": [
    "# test canonical dictionary\n",
    "multiple_renames = funcy.select_values(lambda x: len(x) > 1, canonical_name_to_other_names_dict)\n",
    "for old_name, new_name in df3.loc[df3.is_rename==1, ['filename_old', 'filename_new']].values:\n",
    "    if old_name in canonical_name_to_other_names_dict:\n",
    "        if new_name not in canonical_name_to_other_names_dict[old_name]:\n",
    "            res = funcy.select_values(lambda x: new_name in x, multiple_renames)\n",
    "            if len(res) > 1:\n",
    "                print(res)\n",
    "            else:\n",
    "                print('Nothing found')\n",
    "                print(old_name, new_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique filenames, then loop through all unique filenames, if part of a rename, \n",
    "# append to the dict associated with that filename. if entries in that dictionary, if a rename to one of those, associate with that\n",
    "\n",
    "canonical_name_to_other_names_dict[\n",
    "    'test/core-tests/test-init.js'] = ['test/core-tests/test-init.js', 'test/core/both/test-init.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a skeleton of all names in x axis mapped to canonical name (invert dictionary)\n",
    "names_final = dict()\n",
    "\n",
    "for k, v in canonical_name_to_other_names_dict.items():\n",
    "    for i in v:\n",
    "        names_final[i] = k\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map these names onto the data set\n",
    "def get_canonical_name(new_name, old_name, name_dict):\n",
    "    if new_name == '/dev/null':\n",
    "        return name_dict[old_name]\n",
    "    elif old_name == '/dev/null':\n",
    "        return name_dict[new_name]\n",
    "    else:\n",
    "        if old_name in name_dict:\n",
    "            return name_dict[old_name]\n",
    "        elif new_name in name_dict:\n",
    "            return name_dict[new_name]\n",
    "        print('Bad Case ', new_name, old_name)\n",
    "        return new_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['canonical_name'] = df3.apply(\n",
    "    lambda x: get_canonical_name(x['filename_new'], x['filename_old'], names_final), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "I need to deal with the missing filenames - this is an example of a bigger issue, but only 164 at the moment\n",
    "so will continue writing code to generate graphs\n",
    "\n",
    "Also need to fix duplication issue in the database for Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gitlog_parser import is_raw_diff\n",
    "\n",
    "def code_complexity(text_str):\n",
    "    return len(text_str) - len(text_str.lstrip())\n",
    "    \n",
    "def analyze_diff(text_str):\n",
    "    lines = text_str.split('||||')\n",
    "    if len(lines)==1:\n",
    "        lines = text_str.split('|||')\n",
    "    complexity = []\n",
    "    for line in lines:\n",
    "        if line.startswith('+'):\n",
    "            complexity.append(code_complexity(line[1:]))\n",
    "    return complexity\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['code_complexity'] = df3.raw_diff.apply(analyze_diff)\n",
    "df3['code_complexity_max'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.max(x))\n",
    "df3['code_complexity_min'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.min(x))\n",
    "df3['code_complexity_mean'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.mean(x))\n",
    "df3['code_complexity_median'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.median(x))\n",
    "df3['net_change'] = df3.additions - df3.deletions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repo Level Stats\n",
    "\n",
    "total lines of code added\n",
    "total lines of code deleted\n",
    "total number of files add, deleted, renamed\n",
    "total number of files\n",
    "total net code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats = df3.sort_values('timestamp').groupby('timestamp').agg({\n",
    "    'additions': np.sum, 'deletions': np.sum, 'net_change': np.sum,\n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum}).rename(columns={\n",
    "        'additions': 'total_lines_added', \n",
    "        'deletions': 'total_lines_deleted',\n",
    "        'net_change': 'total_lines_code',\n",
    "        'filename_old': 'total_num_unique_files',\n",
    "        'diff_id': 'total_num_edit_locations',\n",
    "         'is_rename': 'total_num_renames',\n",
    "        'is_deletion': 'total_num_deletions',\n",
    "        'is_new': 'total_num_new_files'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a cumulative sum along time axis\n",
    "cum_repo_stats = time_stats.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit level code info\n",
    "calculate amount of net code\n",
    "plot net code, additions, deletions in each change\n",
    "calculate number of file changed in each commit \n",
    "for each commit, also want total number of change in each commit\n",
    "for each commit, try to get average change size\n",
    "\n",
    "\n",
    "### file level\n",
    "cumulative number of changes\n",
    "cumulative number of devs who work on them\n",
    "code churn - number of changes relative to total changes\n",
    "average code change per change to each file\n",
    "\n",
    "### file pairs\n",
    "create count of all files changed together and show which are most commonly changed together\n",
    "\n",
    "\n",
    "### Developer\n",
    "number of changes per developer\n",
    "total lines added/deleted per developer\n",
    "number of created/deleted/renamed files per dev\n",
    "\n",
    "create pairs of devs who work together\n",
    "\n",
    "\n",
    "\n",
    "### Oustanding To Dos\n",
    "need to fix rename fail in database pipelines\n",
    "need to add files to ignore\n",
    "why are tests failing after that change?\n",
    "code complexity count per diff\n",
    "\n",
    "\n",
    "# Questions\n",
    "how to aggregate code complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit level stats\n",
    "    1) number of files changed per commit\n",
    "\n",
    "    2) number of edit locations per commit\n",
    "\n",
    "    3) average code complexity of each diff (note not exactly sure how to aggregate this)\n",
    "    \n",
    "    4) Total number of lines changed\n",
    "    \n",
    "    5) Committer/author same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds = df3.sort_values(['commit_id', 'timestamp']).groupby('commit_id').agg({\n",
    "    'additions': np.sum, 'deletions': np.sum, 'net_change': np.mean,\n",
    "     'timestamp': np.min, 'code_complexity_median': np.mean, \n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_stats = adds.rename(columns={\n",
    "    'additions': 'total_additions', \n",
    "    'deletions': 'total_deletions', 'net_change': 'average_net_change',\n",
    "    'code_complexity_median':'mean_of_code_complexity_median', \n",
    "    'filename_old': 'num_unique_files_changed', 'diff_id': 'num_edit_locations',\n",
    "     'is_rename': 'num_renames', 'is_deletion': 'num_deletions',\n",
    "        'is_new': 'num_new_files'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "devs = df3.sort_values('commit_id').drop_duplicates('commit_id')[[\n",
    "    'commit_id', 'author_id', 'commiter_id', 'author_email', 'commiter_email', 'commit_body',\n",
    "    'git_hash', 'repo_id', 'sha1', 'subject']]\n",
    "commit_stats2 = commit_stats.merge(devs, left_index=True, right_on='commit_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count where developer and ocmmiter are different\n",
    "commit_stats2['diff_dev_commit'] = commit_stats2.apply(\n",
    "    lambda x: 0 if x['author_email']==x['commiter_email'] else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313, 21)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commit_stats2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developer Level Stats\n",
    "\n",
    "    min time of engagement\n",
    "    last time of engagement\n",
    "    collaborators who have commited them\n",
    "    people who have worked on the same file\n",
    "    total lines of code commited as a fraction of total lines of code\n",
    "    average change additions/deletions\n",
    "    number renames\n",
    "    number new files\n",
    "    number deletions\n",
    "    type of first engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths = df3.sort_values(['author_id', 'timestamp']).groupby(['author_id', 'timestamp']).agg({\n",
    "    'additions': np.sum, 'deletions': np.sum, 'net_change': np.mean,\n",
    "    'code_complexity_median': np.mean, \n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum}).rename(columns={\n",
    "        'additions': 'total_additions', \n",
    "        'deletions': 'total_deletions', 'net_change': 'average_net_change',\n",
    "        'code_complexity_median':'mean_of_code_complexity_median', \n",
    "        'filename_old': 'num_unique_files_changed', 'diff_id': 'num_edit_locations',\n",
    "        'is_rename': 'num_renames', 'is_deletion': 'num_deletions',\n",
    "        'is_new': 'num_new_files'})\n",
    "auths['net_code'] = auths['total_additions'] - auths['total_deletions']\n",
    "auths_cum = auths.groupby('author_id').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths2 = auths.merge(cum_repo_stats, left_index=True, right_index=True, how='left')\n",
    "auths2['pct_total_additions'] = auths2['total_additions']/auths2['total_lines_added']\n",
    "auths2['pct_total_deleted'] = auths2['total_deletions']/auths2['total_lines_deleted']\n",
    "auths2['pct_total_lines'] = auths2['net_code']/auths2['total_lines_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths_cum2 = auths_cum.merge(cum_repo_stats, left_index=True, right_index=True, how='left')\n",
    "auths_cum2['pct_total_additions'] = auths_cum2['total_additions']/ auths_cum2['total_lines_added']\n",
    "auths_cum2['pct_total_deleted'] = auths_cum2['total_deletions']/ auths_cum2['total_lines_deleted']\n",
    "auths_cum2['pct_total_lines'] = auths_cum2['net_code']/ auths_cum2['total_lines_code']\n",
    "auths_cum2.columns = ['cum_{}'.format(c) for c in auths_cum2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_time = df3.sort_values(\n",
    "    ['author_id', 'timestamp']).groupby(['author_id']).agg({'timestamp': [np.min, np.max]})\n",
    "auth_time.columns = auth_time.columns.get_level_values(1)\n",
    "auth_time2 = auth_time.rename(columns={\n",
    "    'amin': 'first_author_engagement', 'amax':'last_author_engagement'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths3 = auths2.merge(auth_time2, left_index=True, right_index=True, how='left')\n",
    "auths4 = auths2.merge(auths_cum2, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collaborations stats\n",
    "# for each author, I want to keep track of who commits their diffs\n",
    "# who else works on the same files as them\n",
    "author_comm_pairs = df3[['author_id', 'author_email', 'commiter_id', 'commiter_email', 'timestamp']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for pairs that are different\n",
    "collabs = list(filter(lambda x: x[0]!=x[2], author_comm_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each author, find number of commits\n",
    "collab_df = pd.DataFrame(\n",
    "    collabs, columns=['author_id', 'author_email', 'commiter_id', 'commiter_email', 'timestamp']).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab2 = collab_df[['author_id', 'commiter_id']].sort_values('author_id').drop_duplicates().values\n",
    "d = collections.defaultdict(list)\n",
    "c = collections.defaultdict(list)\n",
    "for auth_id, collab in collab2:\n",
    "    d[auth_id].append(collab)\n",
    "    c[collab].append(auth_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths5 = auths4.merge(\n",
    "    collab_df.groupby('author_id').agg(\n",
    "        {'commiter_id': 'nunique'}).rename(columns={'commiter_id': 'num_different_commiters'}),\n",
    "    left_index=True, right_index=True, how='left').fillna({'num_different_commiters': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auths5.num_different_commiters.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at commit behavior\n",
    "comms = df3.sort_values(['commiter_id', 'timestamp']).groupby(['commiter_id', 'timestamp']).agg({\n",
    "    'code_complexity_median': np.mean, \n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum}).rename(columns={\n",
    "        'code_complexity_median':'mean_of_code_complexity_median', \n",
    "        'filename_old': 'num_unique_files_changed', 'diff_id': 'num_edit_locations',\n",
    "        'is_rename': 'num_renames', 'is_deletion': 'num_deletions',\n",
    "        'is_new': 'num_new_files'})\n",
    "comms['net_code'] = auths['total_additions'] - auths['total_deletions']\n",
    "comms_cum = comms.groupby('commiter_id').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at all developers involved, plot number of authored comms, diffed comms, other collaborators, min auth, max auth, min comm, max comm\n",
    "comm_time = df3.sort_values(\n",
    "    ['commiter_id', 'timestamp']).groupby(['commiter_id']).agg({'timestamp': [np.min, np.max],\n",
    "                                                               'commit_id': 'nunique'})\n",
    "comm_time.columns = comm_time.columns.get_level_values(1)\n",
    "comm_time2 = comm_time.rename(columns={\n",
    "    'amin': 'first_commiter_engagement', 'amax':'last_commiter_engagement', 'nunique': 'num_commiter_commits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_time = df3.sort_values(\n",
    "    ['author_id', 'timestamp']).groupby(['author_id']).agg({\n",
    "    'timestamp': [np.min, np.max], \n",
    "    'commit_id': 'nunique'})\n",
    "auth_time.columns = auth_time.columns.get_level_values(1)\n",
    "auth_time2 = auth_time.rename(columns={\n",
    "    'amin': 'first_author_engagement', 'amax':'last_author_engagement', 'nunique': 'num_authored_commits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge commiter and author time info\n",
    "dev = auth_time2.merge(comm_time2, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['people_who_committer_their_commits'] = dev.index.map(lambda x: d[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['people_who_authored_commits_they_commited'] = dev.index.map(lambda x: c[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_first_engagement(x):\n",
    "    \n",
    "    if pd.isnull(x['first_author_engagement']):\n",
    "        return 'commiter'\n",
    "    if pd.isnull(x['first_author_engagement']):\n",
    "        return 'author'\n",
    "    if x['first_author_engagement'] <= x['first_commiter_engagement']:\n",
    "        return 'author'\n",
    "    return 'commiter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(dev.loc[16, 'first_author_engagement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev['type_first_engagement'] = dev.apply(define_first_engagement, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Level Stats\n",
    "\n",
    " 1) Rate of changes\n",
    " \n",
    " 2) Number of developers who have worked on the file (over time)\n",
    " \n",
    " 3) average size of code per change\n",
    " \n",
    " 4) Other files changed with the file\n",
    " \n",
    " 5) Person who has contributed most to the file\n",
    " \n",
    " 6) Number of file renames\n",
    "    \n",
    " 7) when it appeared\n",
    " \n",
    " 8) orginal author\n",
    " \n",
    " 9) files it is usually changed with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = df3.sort_values(\n",
    "    ['canonical_name', 'timestamp']).groupby(['canonical_name',]).agg({'timestamp': [np.min, np.max]})\n",
    "file_time.columns = file_time.columns.get_level_values(1)\n",
    "file_time2 = file_time.rename(columns={\n",
    "    'amin': 'first_appearance', 'amax':'last_appearance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = df3.sort_values(['canonical_name', 'timestamp']).groupby(\n",
    "    ['canonical_name', 'timestamp']).agg({\n",
    "        'additions': np.sum, 'deletions': np.sum, 'net_change': np.sum,\n",
    "        'diff_id': 'nunique',\n",
    "        'is_rename': np.sum, \n",
    "        'is_deletion': lambda x: any(x)}).rename(columns={\n",
    "        'additions': 'total_additions', \n",
    "        'deletions': 'total_deletions', \n",
    "        'net_change': 'total_net_change',\n",
    "        'is_rename': 'num_renames',\n",
    "        'is_deletion': 'is_deleted',\n",
    "        'diff_id': 'num_changes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first author, first commit, first commiter\n",
    "first_dev = df3[\n",
    "    ['canonical_name', 'timestamp', 'author_id', 'author_email', 'commiter_id', 'commiter_email']].sort_values([\n",
    "        'canonical_name', 'timestamp']).drop_duplicates(subset=['canonical_name'], keep='first').set_index(\n",
    "        'canonical_name').drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_files = files.groupby('canonical_name').cumsum().rename(columns={'total_additions': 'cum_total_additions',\n",
    "                                                        'total_deletions': 'cum_total_deletions',\n",
    "                                                        'num_changes': 'cum_num_changes',\n",
    "                                                        'num_renames': 'cum_num_renames',\n",
    "                                                        'is_deleted': 'cum_is_deleted'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get dev who contributed most, groupby filename, dev\n",
    "# then you can also count distinct developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_files2 = cum_files.merge(first_dev, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['additions', 'commit_id', 'deletions', 'filename_new', 'filename_old',\n",
       "       'filetype', 'diff_id', 'is_deletion', 'is_new', 'is_rename', 'raw_diff',\n",
       "       'author_id', 'commit_body', 'commiter_id', 'date_time', 'git_hash',\n",
       "       'repo_id', 'sha1', 'subject', 'timestamp', 'author_email',\n",
       "       'commiter_email', 'canonical_name', 'code_complexity',\n",
       "       'code_complexity_max', 'code_complexity_min', 'code_complexity_mean',\n",
       "       'code_complexity_median', 'net_change'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_file = df3.sort_values('canonical_filename').groupby('canonical_filename').size()\n",
    "files = df3.groupby('canonical_filename').agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of file creations, deletions and renames\n",
    "file_creations = df3[['timestamp', 'is_deletion', 'is_rename', 'is_new']].set_index(\n",
    "    'timestamp').sort_index().cumsum().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "file_creations.plot()\n",
    "plt.title('Cumulative File Deletions, New Files and Renames')\n",
    "plt.savefig(os.path.join(FIGURE_DIR, 'cumulative_file_changes'.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_files.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary column called filename_groupby that includes other filename when one value is dev/null\n",
    "new_files = (df3['filename_old'] == '/dev/null')\n",
    "df3['groupby_filename'] = df3['filename_old']\n",
    "df3.loc[new_files, 'groupby_filename'] = df3[new_files]['filename_new']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3[pd.isnull(df3['groupby_filename'])]\n",
    "print(df3[df3['groupby_filename'] == ''].shape)\n",
    "print(df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby_filename.value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "df3.groupby_filename.value_counts()[:20].plot(kind='bar')\n",
    "plt.title('Number of Diffs Per File')\n",
    "plt.savefig('Diffs_per_file.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "df3.author_email.value_counts()[:20].plot(kind='bar')\n",
    "plt.title('Number of Diffs Per Author')\n",
    "plt.savefig('Diffs_per_author.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag = df3.groupby(\n",
    "    'groupby_filename').agg({'additions': np.sum, 'deletions': np.sum, 'author_id': 'nunique'})\n",
    "frag2 = frag.rename(columns={\n",
    "    'additions': 'total_additions', 'deletions': 'total_deletions', 'author_id':'unique_contributors'}).sort_values(\n",
    "    'unique_contributors', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag2[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "color = 'tab:red'\n",
    "# ax1.set_xlabel('filename')\n",
    "ax1.set_ylabel('Lines of Code', color=color)\n",
    "lns1 = ax1.plot(frag2[:30].index, frag2[:30].total_additions, 'r-', label='total_additions')\n",
    "lns2 = ax1.plot(frag2[:30].index, frag2[:30].total_deletions, 'g-', label='total_deletions')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.legend()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('number unique contributors', color=color)  # we already handled the x-label with ax1\n",
    "lns3 = ax2.plot(frag2[:30].index, frag2[:30].unique_contributors, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "lns = lns1+lns2+lns3\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc=0)\n",
    "\n",
    "plt.title('Fragmentation')\n",
    "plt.show()\n",
    "fig.savefig('frag.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each filename, groupby filename\n",
    "df2 = df.groupby('filename_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_info = []\n",
    "for fname_new, fname_old, commit_id, _ in filename_tups:\n",
    "    commit_info = session.query(Commit).filter(Commit.id==commit_id).first()\n",
    "    new_info.append((\n",
    "        a, b, commit_info.timestamp,\n",
    "            commit_info.subject, commit_info.commit_body, commit_info.author_id,\n",
    "                commit_info.commiter_id, ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Commit).filter(Commit.id==1529).first().author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Diff).first().__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for each of the files changed, I need to know when a change occured, who changed it, how many insertions, deletions did they do, what commit was it part of, what was the commit \n",
    "# for each of the new filenames I need to know lines of code per day added\n",
    "# then, I need to show that by developer\n",
    "# for code fragementation, I need a running average of total lines of code and the number contributed by different developers\n",
    "# then for code churn I need to calculate number of changes per file \n",
    "# then i want to dvivide dev by first itneraction with repo\n",
    "# for dependence I either need to look at who builds on one, or I need to look at who create repos and does first commits into a project (feature)\n",
    "# for each commit, number of files changed can be a proxy\n",
    "# then, for each dev, I want to be able to check who are the group of people they work on code with or change the same file with\n",
    "# for complexity, \n",
    "\n",
    "## checks\n",
    "# why do some files have no changes?\n",
    "# what can I do to add location?\n",
    "# how can i track renames or deletions\n",
    "# what about the boilerplate is throwing this off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
