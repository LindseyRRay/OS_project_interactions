{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import funcy\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "from dev import (LOCAL_DB, DATA_DIR, DROPBOX_DIR,\n",
    "                     REPO_STATS_FNAME, COMMIT_STATS_FNAME, DEV_COLLAB_FNAME, DEV_CHANGES_FNAME,\n",
    "                 DEV_CONTR_BY_FILE_FNAME,\n",
    "                         FILE_DEV_CUM_STATS_FNAME, FILE_ADJ_MATRIX_FNAME, \n",
    "                 FILE_STATS_FNAME, FILE_DEV_CUM_STATS_FNAME, RAW_FNAME)\n",
    "    \n",
    "from models import Commit, Change, Developer, Diff\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_TO_CSV = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_DIR = os.path.join(DATA_DIR, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import connect_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lraymond/Python/sourcecred_research/sourcecred_research3.6/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "session, engine = connect_to_db(LOCAL_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dict = {\n",
    "    'filename_new': [],\n",
    "    'filename_old': [],\n",
    "    'commit_id': None,\n",
    "    'additions': 0,\n",
    "    'deletions': 0,\n",
    "    'is_deletion': False,\n",
    "    'is_rename': False,\n",
    "    'is_new': False,\n",
    "    'commit_timestamp': None,\n",
    "    'commit_subject': '',\n",
    "    'commit_body': '',\n",
    "    'author_id': None,\n",
    "    'commit_id': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of unique filenames\n",
    "# filename_tups = session.query().with_entities(\n",
    "#     Diff.filename_new, Diff.filename_old, Diff.commit_id, \n",
    "#         Diff.additions, Diff.deletions, Diff.is_deletion, Diff.is_rename, \n",
    "#             Diff.is_new).all()\n",
    "\n",
    "filename_tups = session.query(Diff).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame(list(map(lambda x: x.__dict__, filename_tups)))\n",
    "diff_df.rename(columns={'id': 'diff_id'}, inplace=True)\n",
    "diff_df2 = diff_df.drop_duplicates(subset=['filename_new', 'filename_old', 'commit_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10762, 12)\n",
      "(4739, 12)\n"
     ]
    }
   ],
   "source": [
    "print(diff_df.shape)\n",
    "print(diff_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del diff_df2['_sa_instance_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_df = pd.DataFrame(list(map(lambda x: x.__dict__, session.query(Commit).all())))\n",
    "commits_df.rename(columns={'id': 'commit_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del commits_df['_sa_instance_state']\n",
    "del commits_df['raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4739, 20)\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(left=diff_df2, right=commits_df, on='commit_id')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in author information\n",
    "author_df = pd.DataFrame(list(map(lambda x: x.__dict__, session.query(Developer).all())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del author_df['_sa_instance_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in author\n",
    "auth2 = author_df.rename(columns={'email': 'author_email', 'id': 'author_id'})\n",
    "df2 = df.merge(right=auth2[['author_email', 'author_id']], on='author_id')\n",
    "auth3 = author_df.rename(columns={'email': 'commiter_email', 'id': 'commiter_id'})\n",
    "df3 = df2.merge(right=auth3[['commiter_email', 'commiter_id']], on='commiter_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del auth2, auth3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of renames (98, 22)\n",
      "Total shape  (4739, 22)\n"
     ]
    }
   ],
   "source": [
    "# Here I should create something that looks for file renames and maps old name to new name and creates a new column (stable filename)\n",
    "print('Number of renames', df3.loc[df3.is_rename==True].shape)\n",
    "print('Total shape ', df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the ones with blank files names, search for \"rename from \" to \"rename to\"\n",
    "def find_renames(raw_diff_text):\n",
    "    m = re.search(r'rename from (.+\\.\\w{1,10})(\\|\\|\\|\\||\\|\\|\\|)rename to (.+\\.\\w{1,10})', raw_diff_text)\n",
    "    if m:\n",
    "        res = m.groups()\n",
    "        if len(res) == 3:\n",
    "            return res[0], res[2]\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_filenames = ((df3.is_rename==True) & (df3.filename_new=='') & (df3.filename_old==''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_names = df3['raw_diff'].apply(find_renames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[missing_filenames, 'filename_old'] = replacement_names[missing_filenames].apply(lambda x: x[0])\n",
    "df3.loc[missing_filenames, 'filename_new'] = replacement_names[missing_filenames].apply(lambda x: x[1])\n",
    "df3[['is_deletion', 'is_new', 'is_rename']] = df3[['is_deletion', 'is_new', 'is_rename']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if fillin gin the renames worked\n",
    "# df3.loc[missing_filenames, ['filename_new', 'filename_old', 'raw_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of tups of all filename matches (old, new)\n",
    "filename_tups = df3[['timestamp', 'filename_old', 'filename_new']].sort_values('timestamp').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/test-repo/blocks/1220c0fc/1220c0fc6b49543d7bf04e83d2a5a7cbe72a83e80f9c7bca1abcaa42298a57a33ff5.data tests/repo-example/blocks/1220c0fc/1220c0fc6b49543d7bf04e83d2a5a7cbe72a83e80f9c7bca1abcaa42298a57a33ff5.data\n",
      "tests/repo-example/blocks/1220933b/1220933b41d37fd4508cdff45930dff56baef91c7dc345e73d049ab570abe10dfbb9.data test/go-ipfs-repo/blocks/1220933b/1220933b41d37fd4508cdff45930dff56baef91c7dc345e73d049ab570abe10dfbb9.data\n",
      "test/http-api/test-bitswap.js test/http-api/ipfs-api/test-bitswap.js\n",
      "test/go-ipfs-repo/blocks/1220b0cb/1220b0cba7371f11461f77081b947d837cc9a3b80e182ac123bbd427563e8b167c96.data test/go-ipfs-repo/blocks/CIQLB/CIQLBS5HG4PRCRQ7O4EBXFD5QN6MTI5YBYMCVQJDXPKCOVR6RMLHZFQ.data\n",
      "test/http-api/spec/test-pubsub.js test/http-api/spec/pubsub.js\n",
      "test/http-api/spec/test-config.js test/http-api/spec/config.js\n",
      "test/go-ipfs-repo/blocks/CIQER/CIQERMRAAFXUAUOX3V2DCW7R77FRIVHQ3V5OIPPS3XQBX34KRPNOIRQ.data test/test-data/go-ipfs-repo/blocks/CIQER/CIQERMRAAFXUAUOX3V2DCW7R77FRIVHQ3V5OIPPS3XQBX34KRPNOIRQ.data\n",
      "test/http-api/interface/test-object.js test/http-api/interface/object.js\n",
      "test/http-api/interface/test-files.js test/http-api/interface/files.js\n",
      "test/http-api/interface/test-config.js test/http-api/interface/config.js\n",
      "test/http-api/spec/test-bitswap.js test/http-api/spec/bitswap.js\n",
      "test/http-api/interface/test-block.js test/http-api/interface/block.js\n",
      "test/http-api/over-ipfs-api/test-block.js test/http-api/over-ipfs-api/block.js\n",
      "test/http-api/interface/test-pubsub.js test/http-api/interface/pubsub.js\n",
      "test/http-api/interface/test-swarm.js test/http-api/interface/swarm.js\n",
      "test/test-data/go-ipfs-repo/blocks/CIQHP/CIQHPUVCWD6JA6AFUVD6VA64TGWP67KYA3AIMBUMVWGZ5AQN2L2HSWQ.data test/go-ipfs-repo/blocks/SW/CIQHPUVCWD6JA6AFUVD6VA64TGWP67KYA3AIMBUMVWGZ5AQN2L2HSWQ.data\n",
      "test/go-ipfs-repo/blocks/M4/CIQOLBQZSZAODJGGH6RYYVBUXHTS3SM5EORZDU63LYPEFUAFE4SBM4I.data test/fixtures/go-ipfs-repo/blocks/M4/CIQOLBQZSZAODJGGH6RYYVBUXHTS3SM5EORZDU63LYPEFUAFE4SBM4I.data\n",
      "test/http-api/extra/object.js test/http-api/object.js\n",
      "test/http-api/extra/config.js test/http-api/config.js\n"
     ]
    }
   ],
   "source": [
    "canonical_name_to_other_names_dict = dict()\n",
    "# dict that goes from canonical names to other file names\n",
    "\n",
    "other_name_to_canonical_name_dict = dict()\n",
    "# dict that does from file names to canonical name\n",
    "\n",
    "for time_val, old_name, new_name in filename_tups:\n",
    "    if old_name == '/dev/null':\n",
    "        # this is a file creation\n",
    "        if new_name not in canonical_name_to_other_names_dict:\n",
    "            canonical_name_to_other_names_dict[new_name] = [new_name]\n",
    "            other_name_to_canonical_name_dict[new_name] = new_name\n",
    "            \n",
    "    elif new_name == '/dev/null':\n",
    "        # this is a file deletion\n",
    "        # need to check that it exists in the dict because not totally capturing everything perfectly\n",
    "        if old_name not in other_name_to_canonical_name_dict:\n",
    "            # we haven't done anything with this filename yet\n",
    "            canonical_name_to_other_names_dict[old_name] = [old_name]\n",
    "            other_name_to_canonical_name_dict[old_name] = old_name\n",
    "            \n",
    "    elif new_name==old_name:\n",
    "        # new name is the same as old_name\n",
    "         if new_name not in other_name_to_canonical_name_dict:\n",
    "            # we haven't done anything with this filename yet\n",
    "            canonical_name_to_other_names_dict[new_name] = [new_name]\n",
    "            other_name_to_canonical_name_dict[new_name] = new_name\n",
    "    elif new_name != old_name:\n",
    "        # and neither new name nor old name is dev/null\n",
    "        if old_name in other_name_to_canonical_name_dict:\n",
    "            can_name = other_name_to_canonical_name_dict[old_name]\n",
    "            if can_name not in canonical_name_to_other_names_dict:\n",
    "                can_name = other_name_to_canonical_name_dict[can_name]\n",
    "            canonical_name_to_other_names_dict[can_name].append(new_name)\n",
    "            other_name_to_canonical_name_dict[new_name] = old_name\n",
    "        else:\n",
    "            # this is a weird pair\n",
    "            print(old_name, new_name) \n",
    "            other_name_to_canonical_name_dict[old_name] = old_name\n",
    "            other_name_to_canonical_name_dict[new_name] = old_name\n",
    "            canonical_name_to_other_names_dict[old_name] = [new_name]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing found\n",
      "test/core-tests/test-init.js test/core/both/test-init.js\n",
      "Nothing found\n",
      "examples/basics/index.js /dev/null\n"
     ]
    }
   ],
   "source": [
    "# test canonical dictionary\n",
    "multiple_renames = funcy.select_values(lambda x: len(x) > 1, canonical_name_to_other_names_dict)\n",
    "for old_name, new_name in df3.loc[df3.is_rename==True, ['filename_old', 'filename_new']].values:\n",
    "    if old_name in canonical_name_to_other_names_dict:\n",
    "        if new_name not in canonical_name_to_other_names_dict[old_name]:\n",
    "            res = funcy.select_values(lambda x: new_name in x, multiple_renames)\n",
    "            if len(res) > 1:\n",
    "                print(res)\n",
    "            else:\n",
    "                print('Nothing found')\n",
    "                print(old_name, new_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique filenames, then loop through all unique filenames, if part of a rename, \n",
    "# append to the dict associated with that filename. if entries in that dictionary, if a rename to one of those, associate with that\n",
    "\n",
    "canonical_name_to_other_names_dict[\n",
    "    'test/core-tests/test-init.js'] = ['test/core-tests/test-init.js', 'test/core/both/test-init.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a skeleton of all names in x axis mapped to canonical name (invert dictionary)\n",
    "names_final = dict()\n",
    "\n",
    "for k, v in canonical_name_to_other_names_dict.items():\n",
    "    for i in v:\n",
    "        names_final[i] = k\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map these names onto the data set\n",
    "def get_canonical_name(new_name, old_name, name_dict):\n",
    "    if new_name == '/dev/null':\n",
    "        return name_dict[old_name]\n",
    "    elif old_name == '/dev/null':\n",
    "        return name_dict[new_name]\n",
    "    else:\n",
    "        if old_name in name_dict:\n",
    "            return name_dict[old_name]\n",
    "        elif new_name in name_dict:\n",
    "            return name_dict[new_name]\n",
    "        print('Bad Case ', new_name, old_name)\n",
    "        return new_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['canonical_name'] = df3.apply(\n",
    "    lambda x: get_canonical_name(x['filename_new'], x['filename_old'], names_final), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gitlog_parser import is_raw_diff\n",
    "\n",
    "def code_complexity(text_str):\n",
    "    return len(text_str) - len(text_str.lstrip())\n",
    "    \n",
    "def analyze_diff(text_str):\n",
    "    lines = text_str.split('||||')\n",
    "    if len(lines)==1:\n",
    "        lines = text_str.split('|||')\n",
    "    complexity = []\n",
    "    for line in lines:\n",
    "        if line.startswith('+'):\n",
    "            complexity.append(code_complexity(line[1:]))\n",
    "    return complexity\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['code_complexity'] = df3.raw_diff.apply(analyze_diff)\n",
    "df3['code_complexity_max'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.max(x))\n",
    "df3['code_complexity_min'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.min(x))\n",
    "df3['code_complexity_mean'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.mean(x))\n",
    "df3['code_complexity_median'] = df3.code_complexity.apply(lambda x: 0 if len(x)==0 else np.median(x))\n",
    "df3['net_change'] = df3.additions - df3.deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    df3.to_csv(os.path.join(DROPBOX_DIR, RAW_FNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repo Level Stats\n",
    "\n",
    "* total lines of code added\n",
    "* total lines of code deleted\n",
    "* total number of files add, deleted, renamed\n",
    "* total number of files\n",
    "* total net code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats = df3.sort_values('timestamp').groupby('timestamp').agg({\n",
    "    'additions': np.sum, 'deletions': np.sum, 'net_change': np.sum,\n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum}).rename(columns={\n",
    "        'additions': 'total_lines_added', \n",
    "        'deletions': 'total_lines_deleted',\n",
    "        'net_change': 'total_lines_code',\n",
    "        'filename_old': 'total_num_unique_files',\n",
    "        'diff_id': 'total_num_edit_locations',\n",
    "         'is_rename': 'total_num_renames',\n",
    "        'is_deletion': 'total_num_deletions',\n",
    "        'is_new': 'total_num_new_files'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a cumulative sum along time axis\n",
    "cum_repo_stats = time_stats.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_repo_stats.columns = ['cum_{}'.format(c) for c in cum_repo_stats.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_level = cum_repo_stats.merge(time_stats, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_stats.shape)\n",
    "print(cum_repo_stats.shape)\n",
    "print(repo_level.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    repo_level.to_csv(os.path.join(DROPBOX_DIR, REPO_STATS_FNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit level code info\n",
    "* calculate amount of net code\n",
    "* plot net code, additions, deletions in each change\n",
    "* calculate number of file changed in each commit \n",
    "* for each commit, also want total number of change in each commit\n",
    "* for each commit, try to get average change size\n",
    "\n",
    "\n",
    "### file level\n",
    "* cumulative number of changes\n",
    "* cumulative number of devs who work on them\n",
    "* code churn - number of changes relative to total changes\n",
    "* average code change per change to each file\n",
    "\n",
    "### file pairs\n",
    "* create count of all files changed together and show which are most commonly changed together\n",
    "\n",
    "\n",
    "### Developer\n",
    "* number of changes per developer\n",
    "* total lines added/deleted per developer\n",
    "* number of created/deleted/renamed files per dev\n",
    "* create pairs of devs who work together\n",
    "\n",
    "\n",
    "\n",
    "### Oustanding To Dos\n",
    "* need to fix rename fail in database pipelines\n",
    "* need to add files to ignore\n",
    "* why are tests failing after that change?\n",
    "* code complexity count per diff\n",
    "* are cumulative things being aggregated properly?\n",
    "* How to combine all the data into a useful format/figure out what is important?\n",
    "* What about adding in text from comments on each commit?\n",
    "* Are we missing any files?\n",
    "* What about file dependence?\n",
    "\n",
    "\n",
    "# Questions\n",
    "* how to aggregate code complexity?\n",
    "* What is important\n",
    "* why is net code < 0 \n",
    "* what are the blank filenames?\n",
    "\n",
    "### TO DO\n",
    "I need to deal with the missing filenames - this is an example of a bigger issue, but only 164 at the moment\n",
    "so will continue writing code to generate graphs\n",
    "\n",
    "Also need to fix duplication issue in the database for Diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit level stats\n",
    "    1) number of files changed per commit\n",
    "\n",
    "    2) number of edit locations per commit\n",
    "\n",
    "    3) average code complexity of each diff (note not exactly sure how to aggregate this)\n",
    "    \n",
    "    4) Total number of lines changed\n",
    "    \n",
    "    5) Committer/author same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds = df3.sort_values(['commit_id', 'timestamp']).groupby('commit_id').agg({\n",
    "    'additions': np.sum, 'deletions': np.sum, 'net_change': np.mean,\n",
    "     'timestamp': np.min, 'code_complexity_median': np.mean, \n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_stats = adds.rename(columns={\n",
    "    'additions': 'total_additions', \n",
    "    'deletions': 'total_deletions', 'net_change': 'average_net_change',\n",
    "    'code_complexity_median':'mean_of_code_complexity_median', \n",
    "    'filename_old': 'num_unique_files_changed', 'diff_id': 'num_edit_locations',\n",
    "     'is_rename': 'num_renames', 'is_deletion': 'num_deletions',\n",
    "        'is_new': 'num_new_files'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devs = df3.sort_values('commit_id').drop_duplicates('commit_id')[[\n",
    "    'commit_id', 'author_id', 'commiter_id', 'author_email', 'commiter_email', 'commit_body',\n",
    "    'git_hash', 'repo_id', 'sha1', 'subject']]\n",
    "commit_stats2 = commit_stats.merge(devs, left_index=True, right_on='commit_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count where developer and ocmmiter are different\n",
    "commit_stats2['diff_dev_commit'] = commit_stats2.apply(\n",
    "    lambda x: 0 if x['author_email']==x['commiter_email'] else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    commit_stats2.to_csv(os.path.join(DROPBOX_DIR, COMMIT_STATS_FNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developer Level Stats\n",
    "\n",
    "    * date of first engagement with project by author/commiter\n",
    "    * type of first engagement\n",
    "    * date of last time of engagement with project\n",
    "    * number of different developers they have worked with (other people who have commited their authored diffs and authors whom they have commited as diffs) - both a count and who specifically\n",
    "    * people who have worked on the same file\n",
    "    * The following info is cumulative and static\n",
    "    * total lines of code commited as a fraction of total lines of code\n",
    "    * average change additions/deletions\n",
    "    * number renames\n",
    "    * number new files\n",
    "    * number deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths = df3.sort_values(['author_id', 'timestamp']).groupby(['author_id', 'timestamp']).agg({\n",
    "    'additions': np.sum, 'deletions': np.sum, 'net_change': np.mean,\n",
    "    'code_complexity_median': np.mean, 'code_complexity_max': np.max,\n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum}).rename(columns={\n",
    "        'additions': 'total_additions', \n",
    "        'deletions': 'total_deletions', 'net_change': 'average_net_change',\n",
    "        'code_complexity_median':'mean_of_code_complexity_median', \n",
    "        'filename_old': 'num_unique_files_changed', 'diff_id': 'num_edit_locations',\n",
    "        'is_rename': 'num_renames', 'is_deletion': 'num_deletions',\n",
    "        'is_new': 'num_new_files'})\n",
    "auths['net_code'] = auths['total_additions'] - auths['total_deletions']\n",
    "auths_cum = auths.groupby('author_id').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths2 = auths.merge(cum_repo_stats, left_index=True, right_index=True, how='left')\n",
    "auths2['pct_total_additions'] = auths2['total_additions']/auths2['cum_total_lines_added']\n",
    "auths2['pct_total_deleted'] = auths2['total_deletions']/auths2['cum_total_lines_deleted']\n",
    "auths2['pct_total_lines'] = auths2['net_code']/auths2['cum_total_lines_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths_cum2 = auths_cum.merge(cum_repo_stats, left_index=True, right_index=True, how='left')\n",
    "auths_cum2['pct_total_additions'] = auths_cum2['total_additions']/auths_cum2['cum_total_lines_added']\n",
    "auths_cum2['pct_total_deleted'] = auths_cum2['total_deletions']/auths_cum2['cum_total_lines_deleted']\n",
    "auths_cum2['pct_total_lines'] = auths_cum2['net_code']/auths_cum2['cum_total_lines_code']\n",
    "auths_cum2_short = auths_cum2.drop(columns=[c for c in cum_repo_stats.columns], axis=1)\n",
    "auths_cum2_short.columns = ['cum_{}'.format(c) for c in auths_cum2_short.columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_time = df3.sort_values(\n",
    "    ['author_id', 'timestamp']).groupby(['author_id']).agg({'timestamp': [np.min, np.max]})\n",
    "auth_time.columns = auth_time.columns.get_level_values(1)\n",
    "auth_time2 = auth_time.rename(columns={\n",
    "    'amin': 'first_author_engagement', 'amax':'last_author_engagement'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths3 = auths2.merge(auth_time2, left_index=True, right_index=True, how='left')\n",
    "auths4 = auths2.merge(auths_cum2_short, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collaborations stats\n",
    "# for each author, I want to keep track of who commits their diffs\n",
    "# who else works on the same files as them\n",
    "author_comm_pairs = df3[['author_id', 'author_email', 'commiter_id', 'commiter_email', 'timestamp']].values\n",
    "\n",
    "# filter for pairs that are different\n",
    "collabs = list(filter(lambda x: x[0]!=x[2], author_comm_pairs))\n",
    "\n",
    "# for each author, find number of commits\n",
    "collab_df = pd.DataFrame(\n",
    "    collabs, columns=['author_id', 'author_email', 'commiter_id', 'commiter_email', 'timestamp']).drop_duplicates()\n",
    "\n",
    "\n",
    "collab2 = collab_df[['author_id', 'commiter_id']].sort_values('author_id').drop_duplicates().values\n",
    "d = collections.defaultdict(list)\n",
    "c = collections.defaultdict(list)\n",
    "for auth_id, collab in collab2:\n",
    "    d[auth_id].append(collab)\n",
    "    c[collab].append(auth_id)\n",
    "    \n",
    "\n",
    "auths5 = auths4.merge(\n",
    "    collab_df.groupby('author_id').agg(\n",
    "        {'commiter_id': 'nunique'}).rename(columns={'commiter_id': 'num_different_commiters'}),\n",
    "    left_index=True, right_index=True, how='left').fillna({'num_different_commiters': 0})\n",
    "auths5 = auths5.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auths5.num_different_commiters.min())\n",
    "print(auths5.num_different_commiters.max())\n",
    "print(auths5.num_different_commiters.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at commit behavior\n",
    "comms = df3.sort_values(['commiter_id', 'timestamp']).groupby(['commiter_id', 'timestamp']).agg({\n",
    "    'code_complexity_median': np.mean, \n",
    "    'filename_old': 'nunique', 'diff_id': 'count', \n",
    "    'is_rename': np.sum, 'is_deletion': np.sum, 'is_new': np.sum}).rename(columns={\n",
    "        'code_complexity_median':'mean_of_code_complexity_median',  'code_complexity_max': np.max,\n",
    "        'filename_old': 'num_unique_files_changed', 'diff_id': 'num_edit_locations',\n",
    "        'is_rename': 'num_renames', 'is_deletion': 'num_deletions',\n",
    "        'is_new': 'num_new_files'})\n",
    "comms['net_code'] = auths['total_additions'] - auths['total_deletions']\n",
    "comms_cum = comms.groupby('commiter_id').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now look at all developers involved, plot number of authored comms, diffed comms, other collaborators, min auth, max auth, min comm, max comm\n",
    "comm_time = df3.sort_values(\n",
    "    ['commiter_id', 'timestamp']).groupby(['commiter_id']).agg({'timestamp': [np.min, np.max],\n",
    "                                                               'commit_id': 'nunique'})\n",
    "comm_time.columns = comm_time.columns.get_level_values(1)\n",
    "comm_time2 = comm_time.rename(columns={\n",
    "    'amin': 'first_commiter_engagement', 'amax':'last_commiter_engagement', 'nunique': 'num_commiter_commits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_time = df3.sort_values(\n",
    "    ['author_id', 'timestamp']).groupby(['author_id']).agg({\n",
    "    'timestamp': [np.min, np.max], \n",
    "    'commit_id': 'nunique'})\n",
    "auth_time.columns = auth_time.columns.get_level_values(1)\n",
    "auth_time2 = auth_time.rename(columns={\n",
    "    'amin': 'first_author_engagement', 'amax':'last_author_engagement', 'nunique': 'num_authored_commits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge commiter and author time info\n",
    "dev = auth_time2.merge(comm_time2, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "dev['people_who_committer_their_commits'] = dev.index.map(lambda x: d[x])\n",
    "\n",
    "dev['people_who_authored_commits_they_commited'] = dev.index.map(lambda x: c[x])\n",
    "\n",
    "auth_merge = author_df[['email', 'id', 'name']].rename(columns={'id': 'author_id'}).set_index('author_id')\n",
    "\n",
    "dev2 = dev.merge(auth_merge, left_index=True, right_index=True, how='left')\n",
    "\n",
    "dev2.index.name = 'author_id'\n",
    "dev2 = dev2.sort_index()\n",
    "\n",
    "devs2 = dev.merge(auths5, left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add data set on group of people who only committed stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(devs2.shape)\n",
    "print(auths5.shape)\n",
    "print(dev.shape)\n",
    "print(dev.index.unique().shape)\n",
    "print(devs2.index.get_level_values(0).unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to CSV\n",
    "if EXPORT_TO_CSV:\n",
    "    auths5.to_csv(os.path.join(DROPBOX_DIR, DEV_CHANGES_FNAME))\n",
    "    dev2.to_csv(os.path.join(DROPBOX_DIR, DEV_COLLAB_FNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Level Stats\n",
    "\n",
    "\n",
    " 1) time of first appearance, last appearance\n",
    " \n",
    " 2) Number of developers who have worked on the file (over time)\n",
    " \n",
    " 3) average size of code per change\n",
    " \n",
    " 4) Other files changed with the file (adjacency matrix of changes)\n",
    " \n",
    " 5) Person who has contributed most to the file\n",
    " \n",
    " 6) Number of file renames\n",
    "    \n",
    " 7) when it appeared\n",
    " \n",
    " 8) orginal author\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = df3.sort_values(\n",
    "    ['canonical_name', 'timestamp']).groupby(['canonical_name',]).agg({'timestamp': [np.min, np.max]})\n",
    "file_time.columns = file_time.columns.get_level_values(1)\n",
    "file_time2 = file_time.rename(columns={\n",
    "    'amin': 'first_appearance', 'amax':'last_appearance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = df3.sort_values(['canonical_name', 'timestamp']).groupby(\n",
    "    ['canonical_name', 'timestamp']).agg({\n",
    "        'additions': np.sum, 'deletions': np.sum, 'net_change': np.sum,\n",
    "        'diff_id': 'nunique',\n",
    "        'is_rename': np.sum, \n",
    "        'is_deletion': lambda x: any(x)}).rename(columns={\n",
    "        'additions': 'total_additions', \n",
    "        'deletions': 'total_deletions', \n",
    "        'net_change': 'total_net_change',\n",
    "        'is_rename': 'num_renames',\n",
    "        'is_deletion': 'is_deleted',\n",
    "        'diff_id': 'num_changes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first author, first commit, first commiter\n",
    "first_dev = df3[\n",
    "    ['canonical_name', 'timestamp', 'author_id', 'author_email', 'commiter_id', 'commiter_email']].sort_values([\n",
    "        'canonical_name', 'timestamp']).drop_duplicates(subset=['canonical_name'], keep='first').set_index(\n",
    "        'canonical_name').drop('timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_files = files.groupby('canonical_name').cumsum().rename(columns={'total_additions': 'cum_total_additions',\n",
    "                                                        'total_deletions': 'cum_total_deletions',\n",
    "                                                        'num_changes': 'cum_num_changes',\n",
    "                                                        'num_renames': 'cum_num_renames',\n",
    "                                                        'is_deleted': 'cum_is_deleted'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_files2 = cum_files.merge(first_dev, left_index=True, right_index=True, how='left')\n",
    "print(first_dev.shape)\n",
    "print(cum_files.shape)\n",
    "print(cum_files2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    cum_files2.to_csv(os.path.join(DROPBOX_DIR, FILE_CUM_CHANGES_FNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get dev who contributed most, groupby filename, dev\n",
    "# then you can also count distinct developers\n",
    "# this gets total number of unique developers and commiters per file\n",
    "\n",
    "# this will be merged into first_dev data set that tells us about the first author and commiter for each file\n",
    "file_dev = df3.sort_values(\n",
    "    ['canonical_name', 'author_id', 'timestamp']).groupby(['canonical_name']).agg({\n",
    "    'author_id': 'nunique', 'commiter_id': 'nunique', \n",
    "    'additions': np.sum, 'deletions': np.sum,  'net_change': np.sum, 'code_complexity_max': np.max}\n",
    "        ).rename(columns={\n",
    "        'author_id': 'num_unique_devs','commiter_id': 'num_unique_commiters',\n",
    "        'additions': 'total_additions', 'deletions': 'total_deletions',\n",
    "        'net_change': 'total_code', 'code_complexity_max': 'max_complexity'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file, also want to break out developers by percentage of code written\n",
    "# groupby filename, developer, sum total lines of code (cum sum), then cum sum by dev and \n",
    "# then calculate a percentage\n",
    "\n",
    "\n",
    "file_dev_lines = df3.sort_values(\n",
    "    ['canonical_name', 'author_id', 'timestamp']).groupby(['canonical_name', 'author_id']).agg({\n",
    "    'additions': np.sum, 'deletions': np.sum,  'net_change': np.sum, 'code_complexity_max': np.max,\n",
    "    'timestamp': np.max})\n",
    "file_dev_code_breakdown = file_dev_lines.merge(right=file_dev, left_index=True, right_index=True)\n",
    "print(file_dev.shape)\n",
    "print(file_dev_lines.shape)\n",
    "print(file_dev_code_breakdown.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    file_dev_code_breakdown.to_csv(os.path.join(DROPBOX_DIR, FILE_STATS_FNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count of number of solo authored files for each developer\n",
    "# also average age of the files they have worked on\n",
    "# also average number of people on the files they have worked on\n",
    "file_stats = file_dev_lines.reset_index('author_id').groupby(['canonical_name']).agg({'author_id': 'count', 'additions': np.sum, 'deletions': np.sum, \n",
    "                                               'code_complexity_max': np.max, 'timestamp': np.max}).rename(\n",
    "    columns={'author_id': 'distinct_authors', 'additions': 'total_additions', 'deletions': 'total_deletions', \n",
    "            'code_complexity_max': 'file_level_code_complexity_max', 'timestamp': 'time_last_change'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stats2 = file_dev_lines.merge(file_stats, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "file_stats2['solo_authored'] = 0\n",
    "file_stats2.loc[file_stats2.distinct_authors==1, 'solo_authored'] = 1\n",
    "\n",
    "file_stats2['authored_max_complexity'] = 0\n",
    "file_stats2.loc[((file_stats2.code_complexity_max==file_stats2.file_level_code_complexity_max) & (\n",
    "    file_stats2.file_level_code_complexity_max > 0)), 'authored_max_complexity'] = 1\n",
    "\n",
    "file_stats2['pct_additions'] = file_stats2['additions']*100.0/file_stats2['total_additions']\n",
    "file_stats2['pct_deletions'] = file_stats2['deletions']*100.0/file_stats2['total_deletions']\n",
    "file_stats2[['pct_additions', 'pct_deletions']] = file_stats2[['pct_additions', 'pct_deletions']].fillna(0)\n",
    "\n",
    "file_stats2['more_than_50pct_additions'] = file_stats2.pct_additions.apply(lambda x: 1 if x > 50 else 0)\n",
    "file_stats2['more_than_50pct_deletions'] = file_stats2.pct_deletions.apply(lambda x: 1 if x > 50 else 0)\n",
    "\n",
    "file_stats2['last_change'] = file_stats2.apply(lambda x: 1 if x.timestamp==x.time_last_change else 0, axis=1)\n",
    "file_stats2['file_age'] = file_stats2['time_last_change'].apply(lambda x: (pd.datetime.now() - x).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    file_stats2.to_csv(os.path.join(DROPBOX_DIR, DEV_CONTR_BY_FILE_FNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same analysis, but cumulative over time\n",
    "cum_code_by_dev = df3.sort_values(\n",
    "    ['canonical_name', 'author_id', 'timestamp',]).groupby(\n",
    "    ['canonical_name', 'author_id', 'timestamp']).agg(\n",
    "    {'additions': np.sum, 'deletions': np.sum, 'net_change': np.sum}).groupby(\n",
    "    ['canonical_name', 'author_id']).cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    cum_code_by_dev.to_csv(os.path.join(DROPBOX_DIR, FILE_DEV_CUM_STATS_FNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of files that are usually changed together \n",
    "# grouped by commit id, get all canonical filename, create a matrix keeping count of times changed together\n",
    "list_file_combos = []\n",
    "\n",
    "gr = df3.sort_values(\n",
    "    ['commit_id','timestamp', 'canonical_name'])[['commit_id', 'canonical_name']].groupby(['commit_id'])\n",
    "\n",
    "for com_id, assoc_df in gr:\n",
    "    list_file_combos.extend(list(itertools.combinations(assoc_df['canonical_name'].values, r=2)))\n",
    "list_file_combos2 = list(filter(None, list_file_combos))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filenames = set(funcy.flatten(list_file_combos2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = pd.Categorical(list(map(funcy.first, list_file_combos2)), categories=unique_filenames)\n",
    "arr2 = pd.Categorical(list(map(funcy.second, list_file_combos2)), categories=unique_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix_filechanges = pd.crosstab(arr1, arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TO_CSV:\n",
    "    adjacency_matrix_filechanges.to_csv(os.path.join(DROPBOX_DIR, FILE_ADJ_MATRIX_FNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_file = df3.sort_values('canonical_filename').groupby('canonical_filename').size()\n",
    "files = df3.groupby('canonical_filename').agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = df3.sort_values(['canonical_name', 'timestamp']).groupby(\n",
    "    ['canonical_name', 'timestamp']).agg({\n",
    "        'additions': np.sum, 'deletions': np.sum, 'net_change': np.mean,\n",
    "        'code_complexity_median': np.mean, \n",
    "        'diff_id': 'count',\n",
    "        'is_rename': lambda x: np.sum(list(map(int, x))), \n",
    "        'is_deletion': lambda x: any(x)}).rename(columns={\n",
    "        'additions': 'total_additions', \n",
    "        'deletions': 'total_deletions', \n",
    "        'net_change': 'average_net_change',\n",
    "        'is_rename': 'num_renames',\n",
    "        'is_deletion': 'is_deleted',\n",
    "        'code_complexity_median':'mean_of_code_complexity_median', \n",
    "        'diff_id': 'num_changes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_per_file = df3.sort_values('canonical_filename').groupby('canonical_filename').size()\n",
    "files = df3.groupby('canonical_filename').agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of file creations, deletions and renames\n",
    "file_creations = df3[['timestamp', 'is_deletion', 'is_rename', 'is_new']].set_index(\n",
    "    'timestamp').sort_index().cumsum().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "file_creations.plot()\n",
    "plt.title('Cumulative File Deletions, New Files and Renames')\n",
    "plt.savefig(os.path.join(FIGURE_DIR, 'cumulative_file_changes'.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_files.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary column called filename_groupby that includes other filename when one value is dev/null\n",
    "new_files = (df3['filename_old'] == '/dev/null')\n",
    "df3['groupby_filename'] = df3['filename_old']\n",
    "df3.loc[new_files, 'groupby_filename'] = df3[new_files]['filename_new']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3[pd.isnull(df3['groupby_filename'])]\n",
    "print(df3[df3['groupby_filename'] == ''].shape)\n",
    "print(df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby_filename.value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "df3.groupby_filename.value_counts()[:20].plot(kind='bar')\n",
    "plt.title('Number of Diffs Per File')\n",
    "plt.savefig('Diffs_per_file.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "df3.author_email.value_counts()[:20].plot(kind='bar')\n",
    "plt.title('Number of Diffs Per Author')\n",
    "plt.savefig('Diffs_per_author.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag = df3.groupby(\n",
    "    'groupby_filename').agg({'additions': np.sum, 'deletions': np.sum, 'author_id': 'nunique'})\n",
    "frag2 = frag.rename(columns={\n",
    "    'additions': 'total_additions', 'deletions': 'total_deletions', 'author_id':'unique_contributors'}).sort_values(\n",
    "    'unique_contributors', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag2[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "color = 'tab:red'\n",
    "# ax1.set_xlabel('filename')\n",
    "ax1.set_ylabel('Lines of Code', color=color)\n",
    "lns1 = ax1.plot(frag2[:30].index, frag2[:30].total_additions, 'r-', label='total_additions')\n",
    "lns2 = ax1.plot(frag2[:30].index, frag2[:30].total_deletions, 'g-', label='total_deletions')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.legend()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('number unique contributors', color=color)  # we already handled the x-label with ax1\n",
    "lns3 = ax2.plot(frag2[:30].index, frag2[:30].unique_contributors, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "lns = lns1+lns2+lns3\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc=0)\n",
    "\n",
    "plt.title('Fragmentation')\n",
    "plt.show()\n",
    "fig.savefig('frag.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each filename, groupby filename\n",
    "df2 = df.groupby('filename_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_info = []\n",
    "for fname_new, fname_old, commit_id, _ in filename_tups:\n",
    "    commit_info = session.query(Commit).filter(Commit.id==commit_id).first()\n",
    "    new_info.append((\n",
    "        a, b, commit_info.timestamp,\n",
    "            commit_info.subject, commit_info.commit_body, commit_info.author_id,\n",
    "                commit_info.commiter_id, ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Commit).filter(Commit.id==1529).first().author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.query(Diff).first().__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for each of the files changed, I need to know when a change occured, who changed it, how many insertions, deletions did they do, what commit was it part of, what was the commit \n",
    "# for each of the new filenames I need to know lines of code per day added\n",
    "# then, I need to show that by developer\n",
    "# for code fragementation, I need a running average of total lines of code and the number contributed by different developers\n",
    "# then for code churn I need to calculate number of changes per file \n",
    "# then i want to dvivide dev by first itneraction with repo\n",
    "# for dependence I either need to look at who builds on one, or I need to look at who create repos and does first commits into a project (feature)\n",
    "# for each commit, number of files changed can be a proxy\n",
    "# then, for each dev, I want to be able to check who are the group of people they work on code with or change the same file with\n",
    "# for complexity, \n",
    "\n",
    "## checks\n",
    "# why do some files have no changes?\n",
    "# what can I do to add location?\n",
    "# how can i track renames or deletions\n",
    "# what about the boilerplate is throwing this off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
